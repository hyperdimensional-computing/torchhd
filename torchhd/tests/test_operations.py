#
# MIT License
#
# Copyright (c) 2023 Mike Heddes, Igor Nunes, Pere Verg√©s, Denis Kleyko, and Danny Abraham
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#
import pytest
import torch
import torchhd
from torchhd import functional

from .utils import (
    torch_dtypes,
    vsa_tensors,
    supported_dtype,
)


class TestBind:
    @pytest.mark.parametrize("vsa", vsa_tensors)
    @pytest.mark.parametrize("dtype", torch_dtypes)
    def test_value(self, vsa, dtype):
        if not supported_dtype(dtype, vsa):
            return

        if vsa == "BSBC":
            hv = functional.empty(2, 10, vsa, dtype=dtype, block_size=1024)
        else:
            hv = functional.empty(2, 16, vsa, dtype=dtype)
        res = functional.bind(hv[0], hv[1])
        if vsa == "BSC":
            assert torch.all(res == torch.logical_xor(hv[0], hv[1])).item()
        elif vsa == "FHRR" or vsa == "MAP" or vsa == "VTB":
            assert torch.all(res == torch.mul(hv[0], hv[1])).item()
        elif vsa == "HRR":
            from torch.fft import fft, ifft

            assert torch.all(res == ifft(torch.mul(fft(hv[0]), fft(hv[1])))).item()
        elif vsa == "BSBC":
            assert torch.all(res == ((hv[0] + hv[1]) % 1024))
        assert dtype == res.dtype

    def test_device(self):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        hv = functional.random(2, 100, device=device)
        res = functional.bind(hv[0], hv[1])

        assert res.dtype == hv.dtype
        assert res.dim() == 1
        assert res.size(0) == 100
        assert torch.all((hv == -1) | (hv == 1)).item(), "values are either -1 or +1"
        assert res.device.type == device.type


class TestBundle:
    @pytest.mark.parametrize("vsa", vsa_tensors)
    @pytest.mark.parametrize("dtype", torch_dtypes)
    def test_value(self, vsa, dtype):
        if not supported_dtype(dtype, vsa):
            return

        if vsa == "BSBC":
            hv = functional.random(2, 10, vsa, dtype=dtype, block_size=1024)
        else:
            hv = functional.random(2, 16, vsa, dtype=dtype)
        res = functional.bundle(hv[0], hv[1])

        if vsa == "BSC":
            for i in range(10):
                assert (res[i].item() == hv[0][i].item()) or (
                    res[i].item() == hv[1][i].item()
                )

        if vsa == "MAP":
            x = torch.tensor([1, 1, -1, -1, 1, 1, 1, 1, -1, -1], dtype=dtype)
            y = torch.tensor([1, 1, -1, -1, -1, -1, -1, -1, 1, -1], dtype=dtype)

            res = functional.bundle(x, y)
            assert torch.all(
                res == torch.tensor([2, 2, -2, -2, 0, 0, 0, 0, 0, -2], dtype=dtype)
            ).item()

        if vsa == "FHRR":
            assert torch.all(res == hv[0].add(hv[1])).item()

        if vsa == "VTB":
            assert torch.all(res == hv[0].add(hv[1])).item()

        if vsa == "BSBC":
            for i in range(10):
                assert (res[i].item() == hv[0][i].item()) or (
                    res[i].item() == hv[1][i].item()
                )

        assert res.dtype == dtype

    def test_device(self):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        hv = functional.random(2, 100, device=device)
        res = functional.bundle(hv[0], hv[1])

        assert res.dtype == hv.dtype
        assert res.dim() == 1
        assert res.size(0) == 100
        assert torch.all((hv <= 2) & (hv >= -2)).item(), "values are between -2 and +2"
        assert res.device.type == device.type


class TestPermute:
    @pytest.mark.parametrize("vsa", vsa_tensors)
    @pytest.mark.parametrize("dtype", torch_dtypes)
    def test_value(self, vsa, dtype):
        if not supported_dtype(dtype, vsa):
            return

        if vsa == "BSBC":
            hv = functional.random(2, 100, vsa, dtype=dtype, block_size=1024)
        else:
            hv = functional.random(2, 100, vsa, dtype=dtype)
        res = functional.permute(hv[0])

        assert res.dtype == hv.dtype
        assert res.dim() == 1
        assert res.size(0) == 100

        if vsa == "BSC":
            assert torch.all((hv == 0) | (hv == 1)).item(), "values are either -1 or +1"
            assert torch.sum(res == hv[0]) != res.size(
                0
            ), "all element must not be the same"

            one_shift = functional.permute(hv[0])
            two_shift = functional.permute(hv[0], shifts=2)
            assert torch.sum(one_shift == two_shift) != res.size(
                0
            ), "all element must not be the same"

        if vsa == "MAP":
            assert torch.all(
                (hv == -1) | (hv == 1)
            ).item(), "values are either -1 or +1"
            assert torch.sum(res == hv[0]) != res.size(
                0
            ), "all element must not be the same"

            one_shift = functional.permute(hv[0])
            two_shift = functional.permute(hv[0], shifts=2)
            assert torch.sum(one_shift == two_shift) != res.size(
                0
            ), "all element must not be the same"

        if vsa == "BSBC":
            hv = functional.random(1, 10000, vsa, dtype=dtype, block_size=1024)
        else:
            hv = functional.random(1, 10000, vsa, dtype=dtype)
        a = functional.permute(hv, shifts=5)
        b = functional.permute(a, shifts=-5)
        assert torch.all(hv == b).item(), "can undo shifts"
        assert res.dtype == dtype

    def test_device(self):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        hv = functional.random(2, 100, device=device)
        res = functional.permute(hv[0])

        assert res.dtype == hv.dtype
        assert res.dim() == 1
        assert res.size(0) == 100
        assert torch.all((hv == -1) | (hv == 1)).item(), "values are either -1 or +1"
        assert res.device.type == device.type


class TestCleanup:
    @pytest.mark.parametrize("vsa", vsa_tensors)
    @pytest.mark.parametrize("dtype", torch_dtypes)
    def test_value(self, vsa, dtype):
        if not supported_dtype(dtype, vsa):
            return

        generator = torch.Generator()
        generator.manual_seed(2147483644)

        if vsa == "BSBC":
            hv = functional.random(
                5, 100, vsa, dtype=dtype, generator=generator, block_size=1024
            )
        else:
            hv = functional.random(5, 100, vsa, dtype=dtype, generator=generator)
        if vsa == "BSBC":
            noise = functional.random(
                1, 100, vsa, dtype=dtype, generator=generator, block_size=1024
            )
        else:
            noise = functional.random(1, 100, vsa, dtype=dtype, generator=generator)
        res = functional.cleanup(functional.bundle(hv[0], noise), hv)
        assert torch.all(hv[0] == res).item()

    @pytest.mark.parametrize("vsa", vsa_tensors)
    @pytest.mark.parametrize("dtype", torch_dtypes)
    def test_threshold(self, vsa, dtype):
        if not supported_dtype(dtype, vsa):
            return

        generator = torch.Generator()
        generator.manual_seed(2147483644)

        if vsa == "BSBC":
            hv = functional.random(
                5, 100, vsa, dtype=dtype, generator=generator, block_size=1024
            )
        else:
            hv = functional.random(5, 100, vsa, dtype=dtype, generator=generator)
        if vsa == "BSBC":
            noise = functional.random(
                1, 100, vsa, dtype=dtype, generator=generator, block_size=1024
            )
        else:
            noise = functional.random(1, 100, vsa, dtype=dtype, generator=generator)
        res = functional.cleanup(functional.bundle(hv[0], noise), hv, threshold=0.3)
        assert torch.all(hv[0] == res).item()

    @pytest.mark.parametrize("vsa", vsa_tensors)
    @pytest.mark.parametrize("dtype", torch_dtypes)
    def test_device(self, vsa, dtype):
        if not supported_dtype(dtype, vsa):
            return

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if vsa == "BSBC":
            hv = functional.random(
                5, 100, vsa, dtype=dtype, device=device, block_size=1024
            )
        else:
            hv = functional.random(5, 100, vsa, dtype=dtype, device=device)
        res = functional.cleanup(hv[0], hv)
        assert res.device.type == device.type


class TestRandsel:
    @pytest.mark.parametrize("vsa", vsa_tensors)
    @pytest.mark.parametrize("dtype", torch_dtypes)
    def test_value(self, vsa, dtype):
        if not supported_dtype(dtype, vsa):
            return
        generator = torch.Generator()
        generator.manual_seed(2147483644)

        if vsa == "BSBC":
            a, b = functional.random(
                2, 1000, vsa, dtype=dtype, generator=generator, block_size=1024
            )
        else:
            a, b = functional.random(2, 1024, vsa, dtype=dtype, generator=generator)
        res = functional.randsel(a, b, p=0, generator=generator)
        assert torch.all(b == res)

        if vsa == "BSBC":
            a, b = functional.random(
                2, 1000, vsa, dtype=dtype, generator=generator, block_size=1024
            )
        else:
            a, b = functional.random(2, 1024, vsa, dtype=dtype, generator=generator)
        res = functional.randsel(a, b, p=1, generator=generator)
        assert torch.all(a == res)

        if vsa == "BSBC":
            a, b = functional.random(
                2, 1000, vsa, dtype=dtype, generator=generator, block_size=1024
            )
        else:
            a, b = functional.random(2, 1024, vsa, dtype=dtype, generator=generator)
        res = functional.randsel(a, b, generator=generator)
        assert torch.all((b == res) | (a == res))
        assert res.dtype == dtype

    @pytest.mark.parametrize("vsa", vsa_tensors)
    @pytest.mark.parametrize("dtype", torch_dtypes)
    def test_device(self, vsa, dtype):
        if not supported_dtype(dtype, vsa):
            return

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if vsa == "BSBC":
            a, b = functional.random(
                2, 100, vsa, dtype=dtype, device=device, block_size=1024
            )
        else:
            a, b = functional.random(2, 100, vsa, dtype=dtype, device=device)
        res = functional.randsel(a, b)

        assert res.dtype == a.dtype
        assert res.dim() == 1
        assert res.size(0) == 100
        assert res.device.type == device.type


class TestMultiRandsel:
    @pytest.mark.parametrize("vsa", vsa_tensors)
    @pytest.mark.parametrize("dtype", torch_dtypes)
    def test_value(self, vsa, dtype):
        if not supported_dtype(dtype, vsa):
            return
        generator = torch.Generator()
        generator.manual_seed(2147483644)

        if vsa == "BSBC":
            x = functional.random(4, 1000, vsa, dtype=dtype, block_size=1024)
        else:
            x = functional.random(4, 1024, vsa, dtype=dtype)

        res = functional.multirandsel(
            x, p=torch.tensor([0.0, 0.0, 1.0, 0.0]), generator=generator
        )
        assert torch.all(x[2] == res)

        if vsa == "BSBC":
            x = functional.random(4, 1000, vsa, dtype=dtype, block_size=1024)
        else:
            x = functional.random(4, 1024, vsa, dtype=dtype)
        res = functional.multirandsel(
            x, p=torch.tensor([0.5, 0.0, 0.5, 0.0]), generator=generator
        )
        assert torch.all((x[0] == res) | (x[2] == res))

        if vsa == "BSBC":
            x = functional.random(4, 1000, vsa, dtype=dtype, block_size=1024)
        else:
            x = functional.random(4, 1024, vsa, dtype=dtype)
        res = functional.multirandsel(x, generator=generator)
        assert torch.all((x[0] == res) | (x[1] == res) | (x[2] == res) | (x[3] == res))
        assert res.dtype == dtype

    def test_device(self):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        x = functional.random(4, 100, device=device)
        res = functional.multirandsel(x)

        assert res.dtype == x.dtype
        assert res.dim() == 1
        assert res.size(0) == 100
        assert res.device.type == device.type


class TestRandomPermute:
    @pytest.mark.parametrize("vsa", vsa_tensors)
    @pytest.mark.parametrize("dtype", torch_dtypes)
    def test_value(self, vsa, dtype):
        if not supported_dtype(dtype, vsa):
            return

        if vsa == "BSBC":
            x = functional.random(4, 100, vsa, block_size=1024)
        else:
            x = functional.random(4, 100, vsa)

        perm = functional.create_random_permute(100)

        assert torch.equal(x, perm(perm(x, 3), -3))
        assert torch.equal(x, perm(x, 0))
        if not torch.is_complex(x):
            assert torch.allclose(x.sort().values, perm(x, 5).sort().values)

    def test_device(self):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        x = functional.random(4, 100, device=device)
        perm = functional.create_random_permute(100)
        perm = perm.to(device)
        assert perm(x).device.type == device.type


class TestResonator:
    def test_shape(self):
        X = functional.random(5, 100)
        Y = functional.random(5, 100)
        Z = functional.random(5, 100)
        domains = torch.stack((X, Y, Z), dim=0)

        x_hat = functional.multiset(X)
        y_hat = functional.multiset(Y)
        z_hat = functional.multiset(Z)
        estimates = torch.stack((x_hat, y_hat, z_hat), dim=0)

        # Create the combined symbol
        s = X[0].bind(Y[1]).bind(Z[3])

        # resonator step
        new_estimates = functional.resonator(s, estimates, domains)
        assert new_estimates.shape == estimates.shape
        assert new_estimates.dtype == estimates.dtype
